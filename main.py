import asyncio
import aiohttp
from collections import deque
import time
import logging
from bs4 import BeautifulSoup
import lxml # for BeautifulSoup. Don't remove
import re
from urllib.parse import urljoin, quote, urlparse
from typing import Tuple, List, Dict, Set
from dataclasses import dataclass
from libzim.writer import Creator, Item, StringProvider, Hint # type: ignore
from pathlib import Path
import traceback
import mimetypes




logging.basicConfig(
    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s"
)

REWRITE_PREFIX = './'

def rewrite_url(url: str, base: str) -> str:
    """
    é‡å†™URLçš„è§„åˆ™å‡½æ•°ã€‚
    è¯¥å‡½æ•°é‡å†™æ‰€æœ‰æœ‰æ•ˆçš„URLï¼Œæ— è®ºå†…éƒ¨è¿˜æ˜¯å¤–éƒ¨ã€‚
    """
    # 1. å¿½ç•¥ç©ºçš„ã€data: URIæˆ–é”šç‚¹é“¾æ¥
    if not url or url.startswith(('data:', '#', 'javascript:')):
        return url

    absolute_original_url = urljoin(base, url)
    parsed_url = urlparse(absolute_original_url)
    
    return REWRITE_PREFIX + f'{parsed_url.netloc}{parsed_url.path}{'#' if parsed_url.fragment else ''}{parsed_url.fragment}{'?' if parsed_url.query else ''}{parsed_url.query}'

def get_mimetype(url: str) -> str:
    """æ ¹æ®URLç¡®å®šMIMEç±»å‹"""
    guess = mimetypes.guess_type(url)[0]
    if guess:
        return guess

    # é»˜è®¤MIMEç±»å‹
    if url.endswith((".html", ".htm")):
        return "text/html"
    elif url.endswith(".css"):
        return "text/css"
    elif url.endswith(".js"):
        return "application/javascript"
    elif url.endswith((".jpg", ".jpeg")):
        return "image/jpeg"
    elif url.endswith(".png"):
        return "image/png"
    elif url.endswith(".gif"):
        return "image/gif"
    elif url.endswith(".svg"):
        return "image/svg+xml"
    elif url.endswith(".ico"):
        return "image/x-icon"
    else:
        return "application/octet-stream"
    
def is_resource_file(url: str) -> bool:
    """åˆ¤æ–­URLæ˜¯å¦ä¸ºèµ„æºæ–‡ä»¶ï¼ˆå›¾ç‰‡ã€CSSã€JSç­‰ï¼‰"""

    return not 'html' in get_mimetype(url) 


def process_html(html_content: str, baseurl: str):
    """
    ä¸»å¤„ç†å‡½æ•°ï¼Œè§£æã€ç»Ÿè®¡å’Œé‡å†™HTMLä¸­çš„æ‰€æœ‰èµ„æºã€‚
    """
    soup = BeautifulSoup(html_content, 'lxml')
    resource_urls = set()
    new_urls = set()
    
    def add_url(url:str):
        if is_resource_file(url):
            resource_urls.add(url)
        else:
            new_urls.add(url)

    # --- 1. å¤„ç†å¸¸è§çš„HTMLæ ‡ç­¾ ---
    tag_attrs = {
        'link': ['href'], 'script': ['src'], 'img': ['src', 'srcset'],
        'audio': ['src'], 'video': ['src', 'poster'], 'source': ['src', 'srcset'],
        'iframe': ['src'], 'embed': ['src'], 'object': ['data'],
    }

    for tag_name, attrs in tag_attrs.items():
        for tag in soup.find_all(tag_name):
            for attr in attrs:
                if tag.has_attr(attr):
                    original_url_attr = tag[attr]
                    
                    if attr == 'srcset':
                        new_srcset_parts = []
                        for part in original_url_attr.split(','):
                            part = part.strip()
                            if not part: continue
                            url_part, *descriptor = part.split(None, 1)
                            add_url(url_part)
                            rewritten = rewrite_url(url_part, baseurl)
                            new_srcset_parts.append(f"{rewritten} {' '.join(descriptor)}")
                        tag[attr] = ', '.join(new_srcset_parts)
                    else:
                        add_url(original_url_attr)
                        tag[attr] = rewrite_url(original_url_attr, baseurl)

    # --- 2. å¤„ç†CSSä¸­çš„url() ---
    def create_rewriter_callback(is_js=False):
        def rewriter(match):
            # JSæ­£åˆ™å¯èƒ½æ•è·å¼•å·ï¼ŒCSSä¸ä¼š
            group_index = 2 if is_js else 1
            quote = match.group(1) if is_js else "'"
            
            original_url = match.group(group_index).strip("'\"")
            add_url(original_url)
            rewritten_url = rewrite_url(original_url, baseurl)
            
            if is_js:
                return f"{quote}{rewritten_url}{quote}"
            else:
                return f"url('{rewritten_url}')"
        return rewriter

    for style_tag in soup.find_all('style'):
        if style_tag.string:
            style_tag.string = re.sub(r'url\((.*?)\)', create_rewriter_callback(), style_tag.string)

    for tag in soup.find_all(style=True):
        tag['style'] = re.sub(r'url\((.*?)\)', create_rewriter_callback(), tag['style'])

    # --- 3. å¤„ç†JavaScriptä¸­çš„é“¾æ¥ (ç®€å•æƒ…å†µ) ---
    for script_tag in soup.find_all('script'):
        if script_tag.string:
            # æ”¹è¿›æ­£åˆ™ä»¥æ›´å¥½åœ°å¤„ç†ç®€å•å­—ç¬¦ä¸²
            new_js = re.sub(r'(["\'])((?:/|https?://|./|../).*?)\1', create_rewriter_callback(is_js=True), script_tag.string)
            script_tag.string = new_js
    logging.info(f"åœ¨{baseurl}æ‰¾åˆ°äº†{len(resource_urls)}ä¸ªèµ„æºé“¾æ¥, {len(new_urls)}ä¸ªé¡µé¢é“¾æ¥")
    return soup.prettify(), sorted(list(resource_urls)), sorted(list(new_urls))

def process_css_content(
    css_content: str, 
    css_base_url: str, 
) -> Tuple[str, List[str]]:
    """
    å¤„ç†CSSå†…å®¹ï¼Œé‡å†™å…¶ä¸­çš„é“¾æ¥ï¼Œå¹¶è¿”å›é‡å†™åçš„å†…å®¹å’Œæ‰¾åˆ°çš„åŸå§‹URLåˆ—è¡¨ã€‚

    Args:
        css_content (str): è¦å¤„ç†çš„CSSä»£ç å­—ç¬¦ä¸²ã€‚
        css_base_url (str): è¯¥CSSæ–‡ä»¶çš„ç»å¯¹URLï¼Œç”¨äºè§£æç›¸å¯¹è·¯å¾„ã€‚

    Returns:
        Tuple[str, List[str]]: 
            - ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯é‡å†™äº†æ‰€æœ‰é“¾æ¥åçš„æ–°CSSå†…å®¹ã€‚
            - ç¬¬äºŒä¸ªå…ƒç´ æ˜¯åœ¨CSSä¸­æ‰¾åˆ°çš„æ‰€æœ‰åŸå§‹URLï¼ˆç»å¯¹è·¯å¾„å½¢å¼ï¼‰çš„åˆ—è¡¨ã€‚
    """
    
    resource_urls = set()
    new_urls = set()
    
    def add_url(url:str):
        if is_resource_file(url):
            resource_urls.add(url)
        else:
            new_urls.add(url)

    # 1. å¤„ç† @import "..." æˆ– @import url(...)
    def import_replacer(match: re.Match) -> str:
        # group(1) åŒ¹é… url(...) ä¸­çš„å†…å®¹, group(2) åŒ¹é… "..." ä¸­çš„å†…å®¹
        original_url = (match.group(1) or match.group(2)).strip("'\"")
        
        # å¿½ç•¥ç©ºURL
        if not original_url:
            return match.group(0)

        # å°†ç›¸å¯¹URLè½¬æ¢ä¸ºç»å¯¹URL
        absolute_url = urljoin(css_base_url, original_url)
        add_url(absolute_url)
        
        # ä» rewrite_map ä¸­æŸ¥æ‰¾é‡å†™åçš„URLï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™ä¿æŒåŸæ ·
        rewritten_url = rewrite_url(original_url, css_base_url)
        
        # è¿”å›æ ‡å‡†æ ¼å¼çš„ @import
        return f'@import url("{rewritten_url}");'

    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… @import è§„åˆ™
    # @import url(path) or @import "path" or @import 'path'
    import_pattern = re.compile(r'@import\s+(?:url\((.*?)\)|["\'](.*?)["\']);?', re.IGNORECASE)
    modified_content = import_pattern.sub(import_replacer, css_content)

    # 2. å¤„ç† url(...)
    def url_replacer(match: re.Match) -> str:
        original_url = match.group(1).strip("'\"")

        # å¿½ç•¥ç©ºçš„ã€data: URI æˆ–é”šç‚¹
        if not original_url or original_url.startswith(('data:', '#')):
            return match.group(0)
            
        absolute_url = urljoin(css_base_url, original_url)
        add_url(absolute_url)

        rewritten_url = rewrite_url(original_url, css_base_url)
        
        # è¿”å›æ ‡å‡†æ ¼å¼çš„ url()
        # æ³¨æ„å¯¹URLä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚å¼•å·ï¼‰è¿›è¡Œè½¬ä¹‰ï¼Œä»¥é˜²ç ´åCSSè¯­æ³•
        escaped_rewritten_url = rewritten_url.replace('"', '\\"')
        return f'url("{escaped_rewritten_url}")'

    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… url() å‡½æ•°
    url_pattern = re.compile(r'url\((.*?)\)', re.IGNORECASE)
    modified_content = url_pattern.sub(url_replacer, modified_content)
    logging.info(f"åœ¨{css_base_url}æ‰¾åˆ°äº†{len(resource_urls)}ä¸ªèµ„æºé“¾æ¥ {len(new_urls)}ä¸ªé¡µé¢é“¾æ¥")
    return modified_content, sorted(list(resource_urls)), sorted(list(new_urls))

@dataclass
class CrawlResult:
    url: str
    remark: str
    content: str | bytes = ''
    mimetype: str = "text/html"

class AsyncHTTPFetcher:
    def __init__(self, max_concurrent=10, rate_limit=5):
        self.max_concurrent = max_concurrent
        self.rate_limit = rate_limit  # æ¯ç§’è¯·æ±‚æ•°é™åˆ¶
        self.queue = asyncio.Queue()
        self.active_tasks = set()
        self.session = None
        self.seen_urls = set()  # URLå»é‡é›†åˆ
        self.last_request_time = deque()  # ç”¨äºé€Ÿç‡é™åˆ¶
        self.running = True
        self.results: List[CrawlResult] = []
        
    async def add_url(self, url):
        """æ·»åŠ æ–°çš„URLåˆ°é˜Ÿåˆ—ï¼ˆå¸¦å»é‡æ£€æŸ¥ï¼‰"""
        if url not in self.seen_urls:
            self.seen_urls.add(url)
            await self.queue.put(url)
            logging.info(f"æ·»åŠ url:{url}")
            return True
        logging.info(f"urlå·²ç»å­˜åœ¨: {url}")
        return False

    async def add_urls(self, urls):
        """æ‰¹é‡æ·»åŠ URL"""
        added = 0
        for url in urls:
            if await self.add_url(url):
                added += 1
        return added

    async def rate_limiter(self):
        """è¯·æ±‚é€Ÿç‡é™åˆ¶å™¨"""
        now = time.monotonic()
        # ç§»é™¤è¶…è¿‡1ç§’çš„è¯·æ±‚æ—¶é—´è®°å½•
        while self.last_request_time and now - self.last_request_time[0] > 1:
            self.last_request_time.popleft()
        
        # å¦‚æœè¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…ç›´åˆ°æœ‰ç©ºæ¡£
        if len(self.last_request_time) >= self.rate_limit:
            wait_time = 1 - (now - self.last_request_time[0])
            await asyncio.sleep(wait_time)
            now = time.monotonic()  # æ›´æ–°å½“å‰æ—¶é—´
        
        # æ·»åŠ æ–°çš„è¯·æ±‚æ—¶é—´
        self.last_request_time.append(now)

    async def fetch(self, url):
        """å¼‚æ­¥è·å–å•ä¸ªURLçš„å†…å®¹"""
        try:
            await self.rate_limiter()  # åº”ç”¨é€Ÿç‡é™åˆ¶
            
            async with self.session.get(url, timeout=10) as response:
                content = await response.text()
                status = response.status
                logging.info(f"âœ… [{status}] Fetched {url} - type: {response.content_type}")
                return url, content, status, response.content_type
        except asyncio.TimeoutError:
            logging.warning(f"â° è·å–è¶…æ—¶:{url}")
            return url, None, "Timeout"
        except Exception as e:
            logging.error(f"âŒ è·å–é”™è¯¯ {url}: {str(e)}")
            return url, None, str(e)

    async def worker(self):
        """å·¥ä½œåç¨‹ï¼Œä»é˜Ÿåˆ—è·å–å¹¶å¤„ç†ä»»åŠ¡"""
        while self.running:
            try:
                url = await asyncio.wait_for(self.queue.get(), timeout=1.0)
                task = asyncio.create_task(self.fetch(url))
                self.active_tasks.add(task)
                
                # æ·»åŠ å›è°ƒå¤„ç†ä»»åŠ¡å®Œæˆåçš„æ¸…ç†
                def callback(future):
                    self.active_tasks.discard(task)
                    self.queue.task_done()
                    
                    # è¿™é‡Œå¯ä»¥æ·»åŠ ç»“æœå¤„ç†é€»è¾‘
                    result = future.result()
                    if result:
                        url, content, status, mimetype = result
                        
                        # ä»å†…å®¹ä¸­æå–æ–°URLå¹¶æ·»åŠ 
                        new_urls = self.parse_links(content, mimetype, url)
                        asyncio.create_task(self.add_urls(new_urls))
                
                task.add_done_callback(callback)
            except asyncio.TimeoutError:
                # é˜Ÿåˆ—ç©ºè¶…æ—¶ï¼Œç»§ç»­å¾ªç¯
                continue

    def parse_links(self, content, mimetype, url):
        """ä»HTMLå†…å®¹ä¸­è§£æé“¾æ¥"""
        if ('html' in mimetype):
            re_content, urls = process_html(content, url)
        elif ('css' in mimetype):
            re_content, urls = process_css_content(content, url)
        else:
            re_content = content
            urls = []        
        
        self.results.append(CrawlResult(remark="success", url=url, content=re_content, mimetype=mimetype))
        return urls

    async def save_to_zim(self, output_file: str, title: str, description: str, creator_meta: str):
        try:
            zim_creator = Creator(Path(output_file))
            
            # å¯åŠ¨Creatorä¸Šä¸‹æ–‡ï¼ˆæ‰€æœ‰æ“ä½œå¿…é¡»åœ¨withå—å†…å®Œæˆï¼‰
            with zim_creator as creator:
                creator.set_mainpath("index.html")
                creator.add_metadata("Title", title)
                creator.add_metadata("Description", description)
                creator.add_metadata("Creator", creator_meta)
                creator.add_metadata("Publisher", "SCP_Spider")
                creator.add_metadata("Language", "zh")

                # ç”Ÿæˆé¦–é¡µç´¢å¼•
                index_content = "<html><head><title>çˆ¬è™«ç»“æœç´¢å¼•</title></head><body>"
                index_content += f"<h1>{title}</h1><p>{description}</p><hr/>"
                index_content += "<h2>å·²çˆ¬å–çš„é¡µé¢</h2><ul>"
                page_count = 0
                file_count = 0

                # å¤„ç†æ‰€æœ‰ç»“æœ
                for result in self.results:

                    # ç”ŸæˆZIMè·¯å¾„ï¼ˆä¸åŸé€»è¾‘ç›¸åŒï¼‰
                    '''parsed_url = urlparse(result.url)
                    path = parsed_url.path
                    if not path or path == "/":
                        path = "/index.html"
                    elif not path.endswith((".html", ".htm", ".css", ".js", ".jpg", ".png", ".gif")):
                        path = path.rstrip("/") + "/index.html"
                    if path.startswith("/"):
                        path = path[1:]
                    if '#' in result.url:
                        path = f'{path}#{result.url.split("#")[-1]}'
                    '''
                    path = result.url.replace('https://', '').replace('http://', '')


                    # åˆ›å»ºæ¡ç›®å¹¶æ·»åŠ 
                    item = Item()
                    item.get_contentprovider = lambda: StringProvider(result.content.encode("utf-8"))
                    item.get_path = lambda: result.url
                    item.get_title = lambda: result.url
                    item.get_mimetype = lambda: result.mimetype
                    item.get_hints = lambda: {Hint.FRONT_ARTICLE: True}
                    logging.info(f'ç”Ÿæˆè·¯å¾„ï¼š{item.get_path()}')
                    creator.add_item(item)  # åœ¨ä¸Šä¸‹æ–‡ä¸­æ·»åŠ 
                    
                    
                    # æ›´æ–°ç´¢å¼•
                    if result.is_resource:
                        file_count += 1
                        index_content += f'<li><a href="{item.get_path()}">{item.get_path()}</a></li>'
                    else:
                        page_count += 1
                        index_content += f'<li><a href="{path}">{result.url}</a></li>'
                    del item # æ˜¯çš„, ç”Ÿå‘½å‘¨æœŸ

                index_content += "</ul></body></html>"

                try:
                    # æ·»åŠ ç´¢å¼•é¡µ
                    index_content_provider = StringProvider(index_content.encode("utf-8"))
                    index_item = Item()
                    index_item.get_path = lambda: f"index.html"
                    index_item.get_title = lambda: "ç´¢å¼•"
                    index_item.get_mimetype = lambda: "text/html"
                    index_item.get_contentprovider = lambda: index_content_provider
                    index_item.get_hints = lambda: {Hint.FRONT_ARTICLE: True}
                    creator.add_item(index_item)  # åœ¨ä¸Šä¸‹æ–‡ä¸­æ·»åŠ 
                except Exception as e:
                    logging.error(f"ç´¢å¼•å·²ç»å­˜åœ¨: {e}")

            # ä¸Šä¸‹æ–‡é€€å‡ºæ—¶ä¼šè‡ªåŠ¨å®ŒæˆZIMæ–‡ä»¶å†™å…¥
            logging.info(f"ZIMæ–‡ä»¶åˆ›å»ºæˆåŠŸï¼š{output_file}ï¼ŒåŒ…å«{page_count}ä¸ªé¡µé¢å’Œ{file_count}ä¸ªèµ„æºæ–‡ä»¶")
            return True
        except Exception as e:
            logging.error(f"åˆ›å»ºZIMæ–‡ä»¶å¤±è´¥ï¼š{e}")
            traceback.print_exc()
            return False
        
        

    async def stop(self):
        """å®‰å…¨åœæ­¢çˆ¬è™«"""
        self.running = False
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        await self.queue.join()
        await asyncio.gather(*self.active_tasks)
        
        if self.session:
            await self.session.close()
        logging.info("ğŸ”´ çˆ¬è™«è¿›ç¨‹é€€å‡º")

    async def run(self, initial_urls=None):
        """å¯åŠ¨çˆ¬è™«ç³»ç»Ÿ"""
        self.session = aiohttp.ClientSession()
        
        # æ·»åŠ åˆå§‹URL
        if initial_urls:
            await self.add_urls(initial_urls)
        
        # åˆ›å»ºå·¥ä½œåç¨‹
        workers = [
            asyncio.create_task(self.worker())
            for _ in range(self.max_concurrent)
        ]
        
        logging.info(f"ğŸš€ å¯åŠ¨ è¿›ç¨‹æ•°:{self.max_concurrent}")
        
        # ä¿æŒä¸»å¾ªç¯è¿è¡Œç›´åˆ°è¢«åœæ­¢
        while self.running:
            await asyncio.sleep(1)
            if self.queue.empty():
                self.running = False
        
        # æ¸…ç†å·¥ä½œ
        await self.stop()
        await asyncio.gather(*workers, return_exceptions=True)
        
        await self.save_to_zim(        
                output_file= f"{initial_urls[0].replace(".", "_").replace(r'/', '_')}-{time.strftime('%Y-%m-%d_%H-%M-%S')}.zim",
                title=f"{initial_urls} çˆ¬è™«å­˜æ¡£",
                description=f"{initial_urls} ç½‘ç«™çš„ç¦»çº¿å­˜æ¡£, ç”±SCP_Spideråˆ›å»º",
                creator_meta="SCP_Spider",)

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    fetcher = AsyncHTTPFetcher(max_concurrent=3, rate_limit=2)
    
    # åˆå§‹URL
    initial_urls = [
        'scp-wiki-cn.wikidot.com/cytus-3',
    ]
    
    # å¯åŠ¨çˆ¬è™«
    fetcher_task = asyncio.create_task(fetcher.run(initial_urls))
    
    # ç­‰å¾…çˆ¬è™«å®Œå…¨åœæ­¢
    await fetcher_task

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nProgram interrupted")